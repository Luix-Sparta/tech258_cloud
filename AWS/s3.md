## S3 and Python S3

**Pre requisite:**
Create a EC2 instance and connect via SSH


1. `sudo apt update -y`: This command updates the list of available packages and their versions, but `-y` flag automatically answers "yes" to all prompts. This ensures that the process runs without needing manual confirmation. Setting `DEBIAN_FRONTEND=noninteractive` prevents any interactive prompts during the update process.

2. `sudo DEBIAN_FRONTEND=noninteractive apt upgrade -y`: This command upgrades all installed packages to their latest versions. Setting `DEBIAN_FRONTEND=noninteractive` prevents any interactive prompts during the upgrade process.

3. `sudo apt-get install python3`: This installs Python 3, the latest version available in your package repository.

4. `sudo apt-get install python3-pip -y`: This installs pip, the package installer for Python, used for installing and managing Python packages.

5. `sudo pip install awscli`: This command installs the AWS Command Line Interface (CLI) using pip, the Python package installer. The AWS CLI allows you to interact with various AWS services from the command line.

6. `alias python=python3`: This command sets an alias so that when you type `python`, it refers to `python3`. This is useful because some systems still use `python` to refer to Python 2, but you want to use Python 3.

After running these commands, you can configure the AWS CLI:

7. `aws configure`: This command initiates the configuration process for the AWS CLI. It will prompt you to enter your AWS Access Key ID, AWS Secret Access Key, default region name, and default output format (usually JSON). You should replace the placeholder values with your actual AWS credentials.

For example:
```Bash
AWS Access Key ID [None]: YOUR_ACCESS_KEY
AWS Secret Access Key [None]: YOUR_SECRET_KEY
Default region name [None]: eu-west-1
Default output format [None]: json

```
Finally, you can list the contents of an S3 bucket using:

8. **List S3 Buckets**:
   - Run `aws s3 ls` to list all S3 buckets in your AWS account.

9. **Create an S3 Bucket**:
   - Run `aws s3 mb s3://tech258-luixhiano-first-bucket` to create a new S3 bucket named "tech258-luixhiano-first-bucket".

10. **Upload Data to S3 Bucket**:
    - Run `echo This is the first line in a test file > test.txt` to create a test file.
    - Run `aws s3 cp test.txt s3://tech258-luixhiano-first-bucket` to upload the test file to the S3 bucket.

11. **Download Data from S3 Bucket**:
    - Run `mkdir downloads` to create a directory for downloaded files.
    - Run `aws s3 sync s3://tech258-luixhiano-first-bucket .` to download all files from the S3 bucket to the current directory.

:warning: **Note**: Deleting objects and buckets is irreversible and can lead to data loss. Use caution when performing delete operations.

12. **Delete Data from S3 Bucket**:
    - Run `aws s3 rm s3://tech258-luixhiano-first-bucket/test.txt` to delete the test file from the S3 bucket.

13. **Delete S3 Bucket**:
    - Run `aws s3 rb s3://tech258-luixhiano-first-bucket --force` to delete the S3 bucket.


Extra Information:

- To delete all objects inside a bucket without confirmation, you can use the `--recursive` flag with the `aws s3 rm` command. For example, `aws s3 rm s3://tech258-luixhiano-first-bucket --recursive`.
- Similarly, to delete a bucket and all its contents without confirmation, you can use the `--force` flag with the `aws s3 rb` command. For example, `aws s3 rb s3://tech258-luixhiano-first-bucket --force`.


### Boto3 Documentation

#### What is Boto3?

**Boto3** is the official AWS SDK for Python. It allows developers to interact with AWS services programmatically using Python scripts. Boto3 provides a high-level API that makes it easy to create, configure, and manage AWS resources, including S3 buckets and objects.

**Key Features**:
- **Simple and Pythonic**: Boto3 offers a Pythonic interface that is easy to understand and use, making it ideal for developers familiar with Python programming.
- **Comprehensive Coverage**: Boto3 provides comprehensive coverage of AWS services, allowing developers to interact with various AWS resources, including compute, storage, databases, and more.
- **Automatic Pagination**: Boto3 automatically handles pagination for API responses, making it easy to iterate over large result sets without having to manage pagination logic manually.
- **Built-in Retry Logic**: Boto3 includes built-in retry logic for handling transient errors, ensuring robustness and reliability when interacting with AWS services.
- **Extensibility**: Boto3 is extensible and allows developers to customize and extend its functionality through plugins and customizations.

#### How to Install Boto3

To install Boto3, you can use pip, the Python package installer. Ensure that you have Python and pip installed on your system, then run the following command:

```python
pip install boto3
```

Once Boto3 is installed, you can start using it to interact with AWS services in your Python scripts.

### Using Python boto3

#### Steps to Create Scripts for S3 Operations

1. **List all the S3 buckets**:
   - Create a Python script file, e.g., `list_buckets.py`.
   - Use Boto3 to create an AWS session and list all S3 buckets using the `list_buckets` method of the `s3` client.
   - Run the script using Python to list all S3 buckets.
   Script Code:
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # List all S3 buckets
   response = s3.list_buckets()

   # Print bucket names
   print("List of S3 Buckets:")
   for bucket in response['Buckets']:
       print(bucket['Name'])
   ```

2. **Create an S3 bucket**:
   - Create another Python script file, e.g., `create_bucket.py`.
   - Use Boto3 to create an AWS session and create a new S3 bucket using the `create_bucket` method of the `s3` client.
   - Specify the desired bucket name in the script.
   - Run the script using Python to create the S3 bucket.
   **Script Code:**
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # Bucket name
   bucket_name = 'tech258-luixhiano-test-boto3'

   # Specify the region where the bucket will be created
   region = 'eu-west-1'  

   
   # Create S3 bucket with the specified region
   s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})
   
   print(f"Bucket '{bucket_name}' created successfully in region '{region}'.")
   ```

3. **Upload data/file to the S3 bucket**:
   - Create a new Python script file, e.g., `upload_file.py`.
   - Use Boto3 to upload a file to an S3 bucket using the `upload_file` method of the `s3` client.
   - Specify the local file path and the destination bucket name in the script.
   - Run the script using Python to upload the file to the S3 bucket.
   **Script Code:**
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # Bucket name
   bucket_name = 'tech258-luixhiano-test-boto3'

   # Local file path
   local_file_path = '/path/to/local/file.txt'

   # S3 object key (file name in S3)
   s3_object_key = 'file.txt'

   # Upload file to S3 bucket
   s3.upload_file(local_file_path, bucket_name, s3_object_key)

   print(f"File '{local_file_path}' uploaded to bucket '{bucket_name}' as '{s3_object_key}'.")
   ```

4. **Download/retrieve content/file from the S3 bucket**:
   - Create another Python script file, e.g., `download_file.py`.
   - Use Boto3 to download a file from an S3 bucket using the `download_file` method of the `s3` client.
   - Specify the bucket name and the object key (file name) in the script.
   - Run the script using Python to download the file from the S3 bucket.
   **Script Code:**
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # Bucket name
   bucket_name = 'tech258-luixhiano-test-boto3'

   # S3 object key (file name in S3)
   s3_object_key = 'file.txt'

   # Local file path to save downloaded file
   local_file_path = '/path/to/save/downloaded/file.txt'

   # Download file from S3 bucket
   s3.download_file(bucket_name, s3_object_key, local_file_path)

   print(f"File '{s3_object_key}' downloaded from bucket '{bucket_name}' to '{local_file_path}'.")
   ```

5. **Delete content/file from the S3 bucket**:
   - Create a Python script file, e.g., `delete_file.py`.
   - Use Boto3 to delete an object (file) from an S3 bucket using the `delete_object` method of the `s3` client.
   - Specify the bucket name and the object key (file name) in the script.
   - Run the script using Python to delete the file from the S3 bucket.
   **Script Code:**
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # Bucket name
   bucket_name = 'tech258-luixhiano-test-boto3'

   # S3 object key (file name in S3)
   s3_object_key = 'file.txt'

   # Delete file from S3 bucket
   s3.delete_object(Bucket=bucket_name, Key=s3_object_key)

   print(f"File '{s3_object_key}' deleted from bucket '{bucket_name}'.")
   ```

6. **Delete the bucket**:
   - Create a final Python script file, e.g., `delete_bucket.py`.
   - Use Boto3 to delete an S3 bucket using the `delete_bucket` method of the `s3` client.
   - Specify the bucket name in the script.
   - Run the script using Python to delete the S3 bucket.
   **Script Code:**
   ```python
   import boto3

   # Create an S3 client
   s3 = boto3.client('s3')

   # Bucket name
   bucket_name = 'tech258-luixhiano-test-boto3'

   # Delete bucket
   s3.delete_bucket(Bucket=bucket_name)

   print(f"Bucket '{bucket_name}' deleted successfully.")
   ```





These steps outline how to create separate Python scripts for each S3 operation using Boto3 and execute them using Python.
